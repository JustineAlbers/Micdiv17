{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crispy CRISPRs\n",
    "David Tian, Audra Devoto, Zoey Werbin and Justine Albers\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "We studied bacterial CRISPR-Cas systems in the Sippewissett Salt Marsh pink berry consortium. We first constructed a phylogenetic tree for the pink berry bacteria. We then matched the CRISPR sequences from full unassembled metagenomic reads of *Bacteroidetes*, *Alphaproteobacteria*, purple sulfur bacteria, and sulfate reducing bacteria found in the pink berries with sequences of potential target phage. We created a database of non-CRISPR reads from the metagenome to search for SNC (spacer-containing non-CRISPR) reads. In addition, we BLAST-searched the SNCs against the full NCBI database filtered for sequences that contain ‘phage’ in the title to determine if they matched phage sequences.\n",
    "\n",
    "###Making phylogenetic tree\n",
    "\n",
    "First, we wanted to **determine how all of the microbial species from the pink berry metagenomic data are related**. In order to do this, we created a phylogenetic tree: \n",
    "\n",
    " - Obtain *recA* sequences for PSB, SRB, the three *Alphaproteobacteria* species, and the five *Bacteroidetes* species from MG-RAST (*recA* is a housekeeping gene used for DNA repair, found across all species).\n",
    " - Get some reference *recA* sequences from NCBI from other related bacteria.\n",
    "\n",
    " - Navigate to the FASTA files of these recA sequences and copy and paste them into Geneious as sequences. \n",
    "\n",
    " - Select all of the sequences and click on ‘Align/Assemble’ to use the ‘Multiple Alignment’ tool to align all of the sequences. \n",
    " - Select the resulting multiple alignment file and click on the ‘Tree’ tool to build the tree. \n",
    "\n",
    " - There are several options / functions that you can select from for the design of your tree. For the purposes of just creating a phylogenetic tree, you can leave many of the defaults alone. \n",
    "\n",
    " - You can select:\n",
    "    -  A genetic distance model\n",
    "    -  A tree building method\n",
    "    -  An outgroup (optional)\n",
    "\n",
    " - Make sure to check the box for ‘Resample Tree’ to include some bootstrap replicates. Choose the option to create a consensus tree and set the support threshold % to zero. \n",
    " \n",
    "### Extracting CRISPR spacers\n",
    "\n",
    "We primarily used a program called CRASS in order to identify CRISPR spacers in the pink berry metagenome. Download the following programs: \n",
    " \n",
    "Apache (https://httpd.apache.org/download.cgi) - a collaborative software development effort aimed at creating a robust, commercial-grade, featureful, and freely-available source code implementation of an HTTP (Web) server\n",
    "\n",
    "Zlib (http://zlib.net/) - software library used for data compression\n",
    "\n",
    "Bedtools (https://github.com/arq5x/bedtools2/releases) - a swiss-army knife of tools for a wide-range of genomics analysis tasks. The most widely-used tools enable genome arithmetic: that is, set theory on the genome. For example, bedtools allows one to intersect, merge, count, complement, and shuffle genomic intervals from multiple files in widely-used genomic file formats such as BAM, BED, GFF/GTF, VCF.\n",
    "\n",
    "CRASS (http://ctskennerton.github.io/crass/) - CRASS is a tool for finding and assembling reads from genomic and metagenomic datasets that contain Clustered Regularly Interspersed Short Palindromic Repeats (CRISPR). CRASS searches through the dataset and identifies reads which contain repeated K-mers that are of a specific length and are separated by a spacer sequence. These possible direct repeats are then curated internally to remove bad matches and then reads containing direct repeats are then outputed for further analysis.\n",
    "\n",
    "**Use CRASS to extract CRISPR arrays from the assembled metagenome:**\n",
    "\n",
    "**Input:** whole-metagenome-assemblies/moleculo-assembly/asmMeta.all.fasta (Moleculo metagenome assembled into contigs)\n",
    "\n",
    "**Output**: crass.crispr file (for information on the .crispr format, see http://ctskennerton.github.io/crass/assets/crispr_spec.pdf), fasta files containing the contigs that each array was found on (each array is labeled as a group, i.e. G1, G2, G3, etc.), graphs (.gv files) for each CRISPR array linking the spacers in their putative order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crass asmMeta.all.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Removing CRISPR-containing reads from metagenome to create BLAST database\n",
    "\n",
    "Creating a new BLAST database that has all of the CRISPR-containing reads removed will allow us to search for sequences in the metagenome that match the CRISPRs. Our first step is to remove all the reads from the metagenome that contain CRISPR reads. Use the following command to **combine all the contigs that were found to have CRISPR arrays on them:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat Group_*.fa > all_crispr_reads.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install BioPython**, a Python package for bioinformatics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sudo pip install biopython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the following script to parse through the metagenome and **remove all contigs containing arrays** (present in all_crispr_reads.fa):\n",
    "\n",
    "Name: fasta_remove.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "from Bio import SeqIO\n",
    "\n",
    "fasta_file = sys.argv[1]  # Input fasta file\n",
    "remove_file = sys.argv[2] # Input wanted file, one gene name per line\n",
    "result_file = sys.argv[3] # Output fasta file\n",
    "\n",
    "remove = set()\n",
    "with open(remove_file) as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line != \"\":\n",
    "            remove.add(line)\n",
    "\n",
    "fasta_sequences = SeqIO.parse(open(fasta_file),'fasta')\n",
    "\n",
    "with open(result_file, \"w\") as f:\n",
    "    for seq in fasta_sequences:\n",
    "        nuc = seq.seq.tostring()\n",
    "        if nuc not in remove and len(nuc) > 0:\n",
    "            SeqIO.write([seq], f, \"fasta\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Call the script.** Arguments are positional, sys.argv[1] = input file, sys.argv[2] = fasta with sequences you wish to remove from the input file, sys.argv[3] = name of output file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python fasta_remove.py asmMeta.fasta all_crispr_reads.fa no_crispr_asmMeta.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the output file is not a linear fasta file. Each line is a set number of characters, which you can see using the head command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head no_crispr_asmMeta.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linearize this file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "awk '/^>/ {printf(\"%s%s\\t\",(N>0?\"\\n\":\"\"),$0);N++;next;} {printf(\"%s\",$0);} END {printf(\"\\n\");}' < no_crispr_asmMeta.fasta > no_crispr_asmMeta.linear.fasta.temp\n",
    "\n",
    "tr \"\\t\" \"\\n\" < no_crispr_asmMeta.linear.fasta.temp > no_crispr_asmMeta.linear.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You may delete the .temp file once you have confirmed that the process worked.)\n",
    "\n",
    "**Check** that the output file does not contain any reads with CRISPR arrays. This should return “no CRISPR arrays found”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crass no_crispr_asmMeta.linear.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create new BLAST database** from the genome with all CRISPR-containing reads removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "makeblastdb -in no_crispr_asmMeta.linear.fasta -parse_seqids -dbtype nucl -out asmMetaNoCRISPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate** a fasta file of all the spacer sequences from all the CRISPR arrays found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crisprtools extract -s crass.crispr > all_spacers.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search** that fasta file against the asmMetaNoCRISPR database you just created using blastn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blastn -db asmMetaNoCRISPR/asmMetaNoCRISPR -query all_spacers.fasta -out spacers_metagenome.classified.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate** out the contigs from asmMetaNoCRISPR that had a match to a spacer (these will be referred to as **SNCs: spacer-containing non crispr contigs**):\n",
    "\n",
    "Create a python script to do this:\n",
    "Name: parse_spacer_classifications.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"spacers_metagenome.classified.csv\", \"r\") as f:\n",
    "\twith open(\"snc_ids.txt\", \"a\") as snc_ids:\n",
    "\t\twith open(\"snc_matches.tsv\", \"a\") as out:\n",
    "\t\t\tfor line in f:\n",
    "\t\t\t\tif line.split(\"= \")[0]==\"Query\":\n",
    "\t\t\t\t\tspacer = line.split(\"= \")[1].rstrip(\"\\n\")\n",
    "\t\t\t\tif line.startswith(\"*\"):\n",
    "\t\t\t\t\tvalue = \"NA\"\n",
    "\t\t\t\t\t#print \"Spacer\", spacer, \"Value \", value\n",
    "\t\t\t\t\tout.write(spacer+\"\\t\"+value+\"\\n\")\n",
    "\t\t\t\telif line.startswith(\">\"):\n",
    "\t\t\t\t\tvalue = line.rstrip('\\n').rstrip(' ')\n",
    "\t\t\t\t\tout.write(spacer+\"\\t\"+value+\"\\n\")\n",
    "\t\t\t\t\tsnc_ids.write(value+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This will create two output files:**\n",
    "\n",
    "snc_ids.txt = a text file of SNC ids\n",
    "\n",
    "snc_matches.tsv = a .tsv file of each spacer, and the SNC id that it matched. \n",
    "\n",
    "If the spacer had no matches in the metagenome, it says NA. This file was later used to make Supplementary Table 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are a bunch of duplicates in this file. **Remove them using sort and uniq:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sort snc_ids.txt | uniq > unique_snc_ids.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find these SNC reads in the metagenome** based on their ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqtk no_crispr_asmMeta.linear.fasta snc_ids.txt > snc_reads.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Great!** Now we have a fasta file of all the SNC reads. \n",
    "\n",
    "###Identifying the origin of the SNC reads using blastx\n",
    "\n",
    "*Note: this never actually worked for us! We were hoping to find out what percentage of the SNC reads had blastx hits to viral proteins, compared to a random sampling of reads from the metagenome. This was modeled off of Fig. S2 from Andersson and Banfield, 2008.*\n",
    "\n",
    "**Launch an AWS EC2 instance.**\n",
    "Select \"AWS Marketplace\". \n",
    "Search NCBI and select the NCBI blast community AMI.\n",
    "Launch the instance, and copy snc_reads.fasta over to the instance.\n",
    "\n",
    "From the instance terminal, **run blastx**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blastx -query snc_reads.fasta -out 'SNC_classifications_blastx.csv' -db nr -outfmt '10 qseqid sseqid sacc slen qstart qend sstart send qseq sseq evalue bitscore length pident qcovs staxid ssciname scomname sblastname sskingdom stitle' -max_target_seqs 5 -num_threads [AS MANY AS YOU HAVE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a very long time. It never finished for us, and we never got to the point of subsampling the metagenome and testing those. If we were to subsample, we would use the qiime script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsample_fasta.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[more info](http://qiime.org/scripts/subsample_fasta.html) on said qiime script\n",
    "\n",
    "###Identifying spacer targets using NCBI\n",
    "\n",
    "To find phage targets, spacer sequences were searched against the entire NCBI database (filtered by entries that had “phage” in the title) using blastn optimized for short sequences. \n",
    "\n",
    "This is all done from a local terminal. \n",
    "\n",
    "**Download** the search strategy specifying blastn search only through sequences that have “phage” in the title, and also optimized for short reads (the spacer regions are very short):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wget https://raw.githubusercontent.com/oddaud/micdiv2017/master/phage_title.asn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run** blastn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blastn -query all_spacers.fasta -out 'Spacer_phage_blastn.classified.csv' -outfmt '10 qseqid sseqid evalue bitscore length pident  qcovs staxid ssciname scomname sblastname sskingdom stitle' -remote -import_search_strategy phage_title.asn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Final data processing steps\n",
    "\n",
    "**Format output file to import into R**\n",
    "\n",
    "Open the Spacer_phage_blastn.classified.csv file in excel. Most of this processing was done by hand, which is a huge pain in the ass.\n",
    "\n",
    " - Add headers. The headers are ‘qseqid sseqid evalue bitscore length pident  qcovs staxid ssciname scomname sblastname sskingdom stitle’\n",
    " - Sort the file by the percent identity column. Delete everything under 100\n",
    " - Sort the file by the coverage column. Delete everything under 80.\n",
    " - Delete the extra columns of the title. This happens because the sequence titles have commas in them, so neither R nor excel knows how to deal with this. Make sure you retain the important title information, such as the fact that it was a phage and its name, but also that the stitle only takes up one column. \n",
    " - Sort the file by qseqid, and then coverage, so that highest coverage is at the top, and qseqids are grouped together. \n",
    " - Once this is complete, save it as a csv called ‘spacer.classified.csv’\n",
    " - Delete the headers\n",
    " - Open the file in R using the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classified <- read.csv(\"path/to/spacer.classified.csv\", header = FALSE, sep =\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Rewrite the file using R (do not know why this is necessary but it is or you run into problems):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write.csv(classified, \"path/to/spacer.classified.CLEAN.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create** a python script to parse through this file and select the top (arbitrary) hit: \n",
    "\n",
    "name: parse_classifications.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = open('phage_spacer_top_hit.csv', 'a')\n",
    "id = \"\"\n",
    "with open('spacer.classified.CLEAN.csv', 'r') as f:\n",
    "\tfor line in f:\n",
    "\t\t#print line\n",
    "\t\tif id==\"\":\n",
    "\t\t\tid = line.split(\",\")[0]\n",
    "\t\t\tout.write(line)\n",
    "\t\tif line.split(\",\")[1] not in id:\n",
    "\t\t\tout.write(line)\n",
    "\t\t\tid = line.split(\",\")[1]\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run this script!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python parse_classifications.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BUT WAIT! There is STILL more data processing...**\n",
    "\n",
    "Making Supplemental Table 1 required a lot of manual processing. \n",
    "\n",
    "**Find the host for each of the arrays:**\n",
    "\n",
    "To do this, map the contig(s) that each array was found on to each of the binned genomes (berry-metagenome/genome_assemblies/*)\n",
    "\n",
    "*This was done using Geneious:* \n",
    "\n",
    " - Import all array contig fasta files (they are labelled as, for example, Group_1_AGAAACACCCCCACGGGCGTGGGGAAGAC.fa)\n",
    " - Import the metagenome assemblies. \n",
    " - Select the first array (Group_1_)\n",
    " - Click “Align/Assemble” then “Map to Reference”. Choose all 13 binned genomes as the reference. \n",
    " - Select “Do not trim”, then “OK”\n",
    " - If there is a hit to any of the genomes, mark it for all the spacers in that array in the excel spreadsheet. If there is not, mark the host as the name of the contig that the array was found on (open the Group_X fasta file to find this). \n",
    "\n",
    "**Sort by groups and spacers:**\n",
    "\n",
    " - Add two columns, one called “group”, one called “spacer”. \n",
    " - Manually go through and fill these in for each spacer. \n",
    " - ex: For G1SP27_Cov_2, the Group is 1 and the spacer is 27.\n",
    " - Save as a csv, called “spacer_classifications.csv”\n",
    "\n",
    "Now the file should look like this: \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/oddaud/micdiv2017/master/table_example.png\"/>\n",
    "\n",
    "**Add in the “unknown” classifications.**\n",
    "Because blastn only returns classified spacers, we are missing a lot of data!\n",
    "\n",
    "**Create a python script** to run through the file all_spacers.fasta and check that all spacers are represented in the classification file:\n",
    "\n",
    "name: add_missing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = open('missing.txt', 'a')\n",
    "with open('spacer_classifications.csv', r) as f:\n",
    "\twith open('all_spacers.fasta', r) as s:\n",
    "\t\tspacer_ids=[]\n",
    "\t\tfor line in f:\n",
    "\t\t\tspacer_id = line.split(\",\")[1]\n",
    "\t\t\tspacer_ids.append(spacer_id)\n",
    "\t\tfor line in s:\n",
    "\t\t\tif line.startswith(\">\"):\n",
    "\t\t\t\tid = line[1:]\n",
    "\t\t\t\tprint id\n",
    "\t\t\t\tif id not in spacer_ids:\n",
    "\t\t\t\t\tout.write(id+\"\\n\")\n",
    "\t\t\t\t\t#print id + \"Not in classified!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output a txt file of all the spacers that are missing from the classified file. Copy these over in excel, and fill in the columns that you can for each one.\n",
    "\n",
    "*OPTIONAL:*\n",
    "\n",
    "Add in information about whether or not each spacer had a hit in the metagenome! Use the file snc_matches.tsv to do this. \n",
    "\n",
    "Save as a csv. Make sure the headers are: \n",
    "id, group, spacer, accession, evalue, unknown, unknown1, pident, taxid, match, cov, host\n",
    "\n",
    "### Creating the network###\n",
    "*(file also attached as generate_networks.R)*\n",
    "\n",
    "In R: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(sharpshootR)\n",
    "library(RColorBrewer)\n",
    "library('Matrix')\n",
    "library(bipartite)\n",
    "library(igraph)\n",
    "\n",
    "#Import Data\n",
    "dataset <- read.table(\"[File path to the final version of the classified csv] \", header = TRUE, sep = ',', fill = TRUE)\n",
    "df <- dataset\n",
    "\n",
    "#Generate a sparse matrix based on the table. Host and match are the two modes of this matrix. \n",
    "A <- spMatrix(nrow=length(unique(df$host)),\n",
    "              ncol=length(unique(df$match)),\n",
    "              i = as.numeric(factor(df$host)),\n",
    "              j = as.numeric(factor(df$match)),\n",
    "              x = rep(1, length(as.numeric(df$host))) )\n",
    "\n",
    "row.names(A) <- levels(factor(df$host))\n",
    "colnames(A) <- levels(factor(df$match))\n",
    "\n",
    "#Turn it into a data frame\n",
    "D <- as.data.frame(as.matrix(A))\n",
    "\n",
    "#Create a graph\n",
    "G <- graph_from_incidence_matrix(A)\n",
    "#Check if it is a bipartite graph!\n",
    "is_bipartite(G)\n",
    "\n",
    "###GENERATE PLOT###\n",
    "##Black represents each of the unassigned contigs. Colors represent the genomes.\n",
    "mycol <- c(\"yellow\", \"#1F78B4\", \"black\",\"black\",\"black\",\"black\",\"black\",\"black\",\n",
    "           \"black\",\"black\",\"black\",\"black\",\"black\",\"black\",\"black\",\"black\",\"black\",\n",
    "           \"black\",\"black\",\"black\",\"black\",\"black\",\"black\",\"red\", \"#33A02C\")\n",
    "\n",
    "#Delete the unknown vertice, this messes up the whole plot \n",
    "#because it looks like a phage called \"unknown\" is attacking all the \n",
    "#genomes simultaneously. In future analysis, these unknown sequences could be included?\n",
    "G <- delete_vertices(G, \"unknown\")\n",
    "\n",
    "##Formatting\n",
    "V(G)$color <- rep('grey', length(V(G)$name))\n",
    "V(G)$color[V(G)$name !=''] <- mycol[V(G)$name !='']\n",
    "\n",
    "plot.igraph(G, vertex.size=2, vertex.label.cex=0.1, vertex.label =NA, edge.width = 0.2)\n",
    "vertex.label = V(G)$name\n",
    "\n",
    "legend(1,1,fill = V(G)$color, legend=V(G)$name[V(G)$name !=''], cex = 0.5, xjust=3.7, yjust = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DONE!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Literature Cited: \n",
    "\n",
    "1) Andersson, A. F., & Banfield, J. F. (2008). Virus population dynamics and acquired virus resistance in natural microbial communities. Science, 320(5879), 1047-1050."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
